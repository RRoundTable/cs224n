{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove\n",
    "\n",
    "- word2vec의 문제인식으로 부터 출발\n",
    "- co-occurence Matrix를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\ryu07\\anaconda3\\envs\\lab\\lib\\site-packages (18.1)\n",
      "Collecting https://github.com/JonathanRaiman/glove/archive/master.zip\n",
      "  Downloading https://github.com/JonathanRaiman/glove/archive/master.zip\n",
      "Requirement already satisfied: cython in c:\\users\\ryu07\\anaconda3\\envs\\lab\\lib\\site-packages (from glove==1.0.1) (0.28.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryu07\\anaconda3\\envs\\lab\\lib\\site-packages (from glove==1.0.1) (1.14.3)\n",
      "Building wheels for collected packages: glove\n",
      "  Running setup.py bdist_wheel for glove: started\n",
      "  Running setup.py bdist_wheel for glove: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ryu07\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-m7_xypdx\\wheels\\d8\\f8\\8d\\8754c24e9f908071d7d0d903591b06d9915b56a491abaae45a\n",
      "Successfully built glove\n",
      "Installing collected packages: glove\n",
      "Successfully installed glove-1.0.1\n"
     ]
    }
   ],
   "source": [
    "# glove 설치하기\n",
    "!python -m pip install --upgrade pip \n",
    "!pip3 install https://github.com/JonathanRaiman/glove/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove의 input data\n",
    "\n",
    "- co-occurence matrix\n",
    "- corpus : 텍스트들로 구성된 리스트\n",
    "\n",
    "- 예시\n",
    "\n",
    "    corpus = [‘이건 무조건 n차각’, ​\t‘미친놈을 동경하는 이유’, \t…‘울었다 웃었다 종 잡을 수 없지만 끝나고 나면 2편을 찾게 되는 영화’]\n",
    "\n",
    "\n",
    "- CountVectorizer: Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \n",
    "    X.toarray() : \n",
    "    [[0 1 1 1 0 0 1 0 1]\n",
    "     [0 2 0 1 0 1 1 0 1]\n",
    "     [1 0 0 1 1 0 1 1 1]\n",
    "     [0 1 1 1 0 0 1 0 1]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I feel hungry' 'tensorflow is very difficult'\n",
      " 'tensorflow is a framework for deep learning'\n",
      " 'tensorflow is very fast changing']\n",
      "[[0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 1 0 1 1]\n",
      " [0 1 0 0 0 1 1 0 1 1 1 0]\n",
      " [1 0 0 1 0 0 0 0 1 0 1 1]]\n",
      "  (10, 0)\t1\n",
      "  (8, 0)\t1\n",
      "  (11, 0)\t1\n",
      "  (3, 0)\t1\n",
      "  (0, 0)\t0\n",
      "  (10, 1)\t1\n",
      "  (8, 1)\t1\n",
      "  (6, 1)\t1\n",
      "  (5, 1)\t1\n",
      "  (1, 1)\t0\n",
      "  (9, 1)\t1\n",
      "  (10, 2)\t1\n",
      "  (8, 2)\t1\n",
      "  (11, 2)\t1\n",
      "  (2, 2)\t0\n",
      "  (10, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (11, 3)\t1\n",
      "  (3, 3)\t0\n",
      "  (0, 3)\t1\n",
      "  (4, 4)\t0\n",
      "  (7, 4)\t1\n",
      "  (10, 5)\t1\n",
      "  (8, 5)\t1\n",
      "  (6, 5)\t1\n",
      "  :\t:\n",
      "  (8, 8)\t0\n",
      "  (11, 8)\t2\n",
      "  (2, 8)\t1\n",
      "  (10, 9)\t1\n",
      "  (8, 9)\t1\n",
      "  (6, 9)\t1\n",
      "  (5, 9)\t1\n",
      "  (1, 9)\t1\n",
      "  (9, 9)\t0\n",
      "  (3, 10)\t1\n",
      "  (0, 10)\t1\n",
      "  (6, 10)\t1\n",
      "  (5, 10)\t1\n",
      "  (1, 10)\t1\n",
      "  (9, 10)\t1\n",
      "  (10, 10)\t0\n",
      "  (8, 10)\t3\n",
      "  (11, 10)\t2\n",
      "  (2, 10)\t1\n",
      "  (3, 11)\t1\n",
      "  (0, 11)\t1\n",
      "  (10, 11)\t2\n",
      "  (8, 11)\t2\n",
      "  (11, 11)\t0\n",
      "  (2, 11)\t1\n",
      "[[0 0 0 1 0 0 0 0 1 0 1 1]\n",
      " [0 0 0 0 0 1 1 0 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 0 1 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 1 1 1 0]\n",
      " [0 1 0 0 0 1 0 0 1 1 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 0 1 1 0 0 1 3 2]\n",
      " [0 1 0 0 0 1 1 0 1 0 1 0]\n",
      " [1 1 1 1 0 1 1 0 3 1 0 2]\n",
      " [1 0 1 1 0 0 0 0 2 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# co-occurrence matrix 만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer=CountVectorizer(min_df=0,ngram_range=(1,1))\n",
    "corpus=[['I','feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "#corpus=[ word for sen in corpus for word in sen ]\n",
    "\n",
    "data=[]\n",
    "# 문장단위로 바꿔준다\n",
    "for lst in corpus:\n",
    "    tmp=\"\"\n",
    "    for word in lst:\n",
    "        tmp+=word\n",
    "        tmp+=\" \"\n",
    "    tmp=tmp[:-1]\n",
    "    data.append(tmp)\n",
    "data=np.asarray(data)\n",
    "print(data)\n",
    "\n",
    "#corpus=[ np.asarray(sen) for sen in corpus ]\n",
    "X=vectorizer.fit_transform(data)\n",
    "\n",
    "print(X.toarray())\n",
    "Xc=X.T*X\n",
    "\n",
    "Xc.setdiag(0)\n",
    "print(Xc)\n",
    "result=Xc.toarray()\n",
    "print(result)\n",
    "dic={}\n",
    "for idx1,word1 in enumerate(result):\n",
    "    tmpdict={}\n",
    "    for idx2, word2 in enumerate(word1):\n",
    "        if word2>0:\n",
    "            tmpdict[idx2]=word2\n",
    "    dic[idx1]=tmpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "vocab=sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "vocab=[word[0] for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, error 0.002\n",
      "epoch 1, error 0.002\n",
      "epoch 2, error 0.002\n",
      "epoch 3, error 0.002\n",
      "epoch 4, error 0.002\n",
      "epoch 5, error 0.002\n",
      "epoch 6, error 0.002\n",
      "epoch 7, error 0.002\n",
      "epoch 8, error 0.002\n",
      "epoch 9, error 0.002\n",
      "epoch 10, error 0.002\n",
      "epoch 11, error 0.002\n",
      "epoch 12, error 0.002\n",
      "epoch 13, error 0.002\n",
      "epoch 14, error 0.002\n",
      "epoch 15, error 0.002\n",
      "epoch 16, error 0.002\n",
      "epoch 17, error 0.002\n",
      "epoch 18, error 0.002\n",
      "epoch 19, error 0.002\n",
      "epoch 20, error 0.002\n",
      "epoch 21, error 0.002\n",
      "epoch 22, error 0.002\n",
      "epoch 23, error 0.002\n",
      "epoch 24, error 0.002\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "import glove\n",
    "model=glove.Glove(dic,d=100,alpha=0.75,x_max=100.0)\n",
    "for epoch in range(25):\n",
    "    err=model.train(batch_size=200, workers=4)\n",
    "    print(\"epoch %d, error %.3f\" % (epoch, err), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00309918  0.00122085 -0.0006237  ...  0.0031917  -0.00442372\n",
      "   0.00170488]\n",
      " [ 0.00266273  0.00207313  0.00296742 ... -0.00397425  0.00303018\n",
      "   0.00446307]\n",
      " [ 0.00477554  0.00381146  0.00127745 ...  0.00322122  0.00128639\n",
      "  -0.00380748]\n",
      " ...\n",
      " [ 0.00392563 -0.00052089 -0.00255559 ... -0.00369455  0.00018913\n",
      "  -0.00021847]\n",
      " [-0.00064451  0.00437919  0.00025642 ... -0.00347495  0.00167016\n",
      "  -0.00132954]\n",
      " [-0.00452926  0.0001251   0.00030331 ...  0.00391254 -0.00049834\n",
      "  -0.00049806]]\n"
     ]
    }
   ],
   "source": [
    "wordvectors=model.W\n",
    "print(wordvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산 함수 : cosine 유사도\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def most_similar(word, vocab, vecs, topn=10):\n",
    "    query=vecs[vocab.index(word)]\n",
    "    result=[]\n",
    "    for idx, vec in enumerate(vecs):\n",
    "        if idx is not vocab.index(word):\n",
    "            result.append((vocab[idx],1-cosine(query,vec)))\n",
    "    result=sorted(result,key=lambda x:x[1], reverse=True)\n",
    "    return result[:topn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deep', 0.17931489283253543),\n",
       " ('learning', 0.058029768758118494),\n",
       " ('hungry', 0.047206340134402947),\n",
       " ('very', 0.034737039386861346),\n",
       " ('difficult', -0.0007956410494136534)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(word='tensorflow', vocab=vocab, \n",
    "             vecs=wordvectors, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
